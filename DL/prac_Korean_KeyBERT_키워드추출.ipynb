{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prac_Korean_KeyBERT_키워드추출",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "9VjDenXDlp-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bc9c0e-f050-4ccc-fdd3-572084e7bb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 20.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=bd9f93933fcd92eee757f63c2c965bca02eccbbf75750eb7a0521da3dfad9678\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 sentence-transformers-2.2.2 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.20.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2-EFCRci8dD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"애플이 정부의 지원을 받는 해킹에서 저명 인사를 보호하기 위해 올가을 아이폰과 아이패드, 맥 컴퓨터 등에 '록다운 모드'를 도입한다고 밝혔다고 경제매체 CNBC가 6일(현지시간) 보도했다.\n",
        "록다운 모드가 되면 아이폰에서 일부 기능이 꺼져 해커가 접근하거나 해킹할 수 있는 기능이 크게 줄면서 아이폰이 스파이웨어에 덜 취약해진다.\n",
        "구체적으로는 아이메시지의 미리 보기 기능, 애플의 웹브라우저인 사파리의 자바스크립트 제한, 신규 설정 프로필 설치 차단, 유선 연결 차단, 화상통화인 페이스타임 등 수신형 서비스 요청 차단 등이다.\n",
        "애플은 앞서 작년 9월 아이폰과 맥 컴퓨터 등에 중대한 보안 취약점이 있다며 긴급 소프트웨어 업데이트를 내놓은 바 있다.\n",
        "이스라엘의 보안기업 NSO그룹이 만든 스파이웨어 '페가수스'를 이용하면 해커들이 클릭 한번 없이도 애플 기기를 감염시켜 카메라나 마이크를 켜고 검색 기록, 문자 메시지나 이메일 내용 등이 노출될 수 있다는 이유에서였다.\n",
        "페가수스 같은 고도의 스파이웨어는 가격이 수천억원에 달해 평범한 해커가 아닌 정부나 경찰 기관 등이 주요 고객이다.\n",
        "애플은 이 사건 뒤 미국 의회와 각국 정부에서 이 문제를 해결하라는 압력을 받아왔다.\n",
        "록다운 모드는 국가의 지원을 받는 해커의 표적이 될 수 있어 극도로 고도의 보안 수준이 요구되는 소수의 이용자를 위한 것이다. 여기에는 고위 정치인이나 언론인, 인권 활동가, 기업 임원 등이 포함된다.\n",
        "애플은 \"대다수 이용자는 고도의 사이버 공격의 희생자가 될 일이 결코 없겠지만 희생자가 될 수 있는 소수의 사람을 보호하기 위해 쉬지 않고 일하겠다\"고 밝혔다.\n",
        "애플은 록다운 모드에 혹시 있을지 모를 보안상 허점을 발견하는 사람에게 최고 200만달러(약 26억원)를 지급하겠다며 '현상금'까지 내걸었다.\"\"\""
      ],
      "metadata": {
        "id": "YfR7ul7RloQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#형태소 분석기를 통해 명사만 추출할 문서 만들기\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_doc = okt.pos(doc)\n",
        "tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
        "print('품사 태깅 10개만 출력 :', tokenized_doc[:10])\n",
        "print('명사 추출 :', tokenized_nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjl51BR_nCSA",
        "outputId": "5344b167-b815-4e34-a1b9-7503314d416f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태깅 10개만 출력 : [('애플', 'Noun'), ('이', 'Josa'), ('정부', 'Noun'), ('의', 'Josa'), ('지원', 'Noun'), ('을', 'Josa'), ('받는', 'Verb'), ('해킹', 'Noun'), ('에서', 'Josa'), ('저명', 'Noun')]\n",
            "명사 추출 : 애플 정부 지원 해킹 저명 인사 보호 위해 올가을 아이폰 아이패드 맥 컴퓨터 등 록 다운 모드 를 도입 경제 매체 현지 시간 보도 록 다운 모드 아이폰 일부 기능 해커 접근 거나 해킹 수 기능 크게 아이폰 스파이웨어 덜 취약 구체 아이메시지 미리 보기 기능 애플 웹브라우저 사파리 자바스크립트 제한 신규 설정 프로필 설치 차단 유선 연결 차단 화상통화 페이스타임 등 신형 서비스 요청 차단 등 애플 앞서 작년 아이폰 맥 컴퓨터 등 보안 취약점 긴급 소프트웨어 업데이트 바 이스라엘 보안 기업 그룹 스파이웨어 페가수스 를 이용 해커 클릭 한번 애플 기기 감염 카메라 마이크 검색 기록 문자 메시지 이메일 내용 등 노출 수 이유 페가수스 고도 스파이웨어 가격 억원 해커 정부 경찰 기관 등 주요 고객 애플 이 사건 뒤 미국 의회 각국 정부 이 문제 해결 압력 록 다운 모드 국가 지원 해커 표적 수 극도 고도 보안 수준 요구 소수 이용자 위 것 여기 고위 정치인 언론인 인권 활동가 기업 임원 등 포함 애플 대다수 이용자 고도 사이버 공격 희생 결코 희생 수 소수 사람 보호 위해 쉬 일 고 애플 록 다운 모드 혹시 보안 점 발견 사람 최고 약 를 지급 현상금 내\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#여기서는 사이킷런의 CountVectorizer를 사용하여 단어 추출 (n-gram 추출 쉬움)\n",
        "\n",
        "n_gram_range = (2,3)\n",
        "\n",
        "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
        "candidates = count.get_feature_names_out()\n",
        "\n",
        "print('gram 개수 :', len(candidates))\n",
        "print('gram 다섯개만 출력 :', candidates[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pstMvO-gn1tb",
        "outputId": "6c05128f-fa09-4f3f-af91-066ccfc77b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram 개수 : 305\n",
            "gram 다섯개만 출력 : ['가격 억원' '가격 억원 해커' '각국 정부' '각국 정부 문제' '감염 카메라']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#이제 문서와 문서로부터 추출한 키워드들을 SBERT를 통해서 수치화한다\n",
        "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')"
      ],
      "metadata": {
        "id": "QqwUz7EJoctF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embedding = model.encode([doc])\n",
        "candidate_embeddings = model.encode(candidates)"
      ],
      "metadata": {
        "id": "GrarEUGSpNi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이제 문서와 가장 유사한 키워드 추출.(문서를 대표하기 위한 좋은 키워드)\n",
        "top_n = 10\n",
        "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "print(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22_FO5G_o6tK",
        "outputId": "d22831eb-178a-4807-f6e8-408693991804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['고객 애플 사건', '일부 기능 해커', '주요 고객 애플', '애플 웹브라우저 사파리', '애플 기기 감염', '애플 웹브라우저', '보호 위해 애플', '애플 정부 지원', '아이폰 스파이웨어 취약', '아이폰 컴퓨터 보안']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distances"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpnzD5vijI0O",
        "outputId": "f4de8db7-38fd-49c5-e6e8-e292495ef284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.86920199e-02,  3.76439542e-01, -1.21156350e-01,\n",
              "        -4.13071141e-02,  9.65741351e-02,  9.71842557e-02,\n",
              "         1.79692477e-01,  3.15241158e-01,  2.23368928e-02,\n",
              "        -1.00282393e-02, -7.56385028e-02,  9.60360467e-03,\n",
              "         1.24056019e-01,  1.05430469e-01,  6.96671903e-02,\n",
              "         2.72480175e-02,  4.19590116e-01,  4.52743232e-01,\n",
              "         1.56958580e-01,  1.74651057e-01,  2.24895537e-01,\n",
              "         2.73111343e-01,  1.85443982e-01,  2.67823428e-01,\n",
              "        -6.40758965e-03,  6.31761104e-02, -6.76746294e-02,\n",
              "        -8.24632794e-02, -8.46472755e-02, -2.43162960e-02,\n",
              "         2.97456253e-02,  4.23026979e-01,  1.32193893e-01,\n",
              "         3.87137569e-02, -1.00299016e-01,  1.26885504e-01,\n",
              "        -9.94826108e-02,  8.43395144e-02,  5.36460727e-02,\n",
              "         2.20051467e-01,  1.61720276e-01,  3.86850178e-01,\n",
              "        -6.07943386e-02,  3.22048962e-01,  4.03184444e-01,\n",
              "         4.34595972e-01, -8.34488347e-02, -4.17380407e-03,\n",
              "        -4.24735062e-02,  2.62084097e-01,  4.33910415e-02,\n",
              "         2.39952430e-02,  2.88407505e-01,  3.51949751e-01,\n",
              "        -6.30697310e-02, -2.79495604e-02,  2.52391100e-02,\n",
              "         3.81731018e-02, -1.07908949e-01, -4.79539745e-02,\n",
              "         1.14387870e-02,  3.56737733e-01, -3.26988101e-02,\n",
              "        -1.06427625e-01, -4.38509658e-02,  2.29876563e-02,\n",
              "         1.49601519e-01, -9.58532840e-03,  4.14026640e-02,\n",
              "        -3.01919375e-02, -4.22996189e-03, -6.85290769e-02,\n",
              "        -7.34891668e-02,  1.95463374e-03,  9.24537629e-02,\n",
              "        -4.05768342e-02,  1.94191843e-01,  3.50226998e-01,\n",
              "         4.09170926e-01, -5.04109412e-02,  6.90468475e-02,\n",
              "        -5.99290766e-02, -2.18654647e-02,  1.50482610e-01,\n",
              "         7.37717152e-02,  3.97421457e-02,  1.61687639e-02,\n",
              "        -4.03237939e-02,  1.05910376e-02, -5.29952198e-02,\n",
              "         6.98716491e-02, -2.27738470e-02, -1.02140158e-02,\n",
              "        -5.95340431e-02,  1.05500840e-01,  2.40898386e-01,\n",
              "         1.45926893e-01,  1.62238479e-01,  9.72876549e-02,\n",
              "         2.27536961e-01,  2.23584533e-01,  2.20608398e-01,\n",
              "         1.60335720e-01, -3.57866921e-02,  4.91052330e-01,\n",
              "         2.08744220e-02, -1.28701344e-01,  3.37719172e-02,\n",
              "         1.37816623e-01,  1.27252936e-02, -1.46998335e-02,\n",
              "        -6.93361461e-03,  2.94475794e-01,  3.18013072e-01,\n",
              "         1.09843560e-01,  4.94066328e-02, -5.64177781e-02,\n",
              "        -9.75804478e-02,  5.74836507e-02,  1.78893313e-01,\n",
              "        -7.23836422e-02,  6.44637793e-02, -7.66806453e-02,\n",
              "         1.67013377e-01, -4.47350144e-02, -1.58942208e-01,\n",
              "         3.38993788e-01,  2.89043039e-01,  8.13904405e-03,\n",
              "         2.46744156e-02,  2.60060996e-01,  1.87520802e-01,\n",
              "         3.35383654e-01,  3.47052813e-01,  5.70717156e-02,\n",
              "         9.34192836e-02, -1.11642763e-01, -1.05008140e-01,\n",
              "        -2.05573142e-02,  1.28570944e-01,  5.68119660e-02,\n",
              "        -2.41887569e-03, -8.59413221e-02,  1.70495868e-01,\n",
              "         3.59055132e-01,  3.01274300e-01,  4.29581285e-01,\n",
              "         5.19811213e-01,  3.73721361e-01,  4.24939483e-01,\n",
              "         3.57049793e-01,  4.45336431e-01,  4.13908303e-01,\n",
              "         5.92105508e-01, -1.16888955e-01, -5.95979206e-02,\n",
              "        -1.18440256e-01,  1.14575922e-01,  4.01532173e-01,\n",
              "         4.62796271e-01,  1.59126461e-01,  3.68816555e-01,\n",
              "         3.21822286e-01,  3.87109548e-01,  3.57487381e-01,\n",
              "         3.67941946e-01,  8.45018029e-02,  4.47557509e-01,\n",
              "         4.62818444e-01,  4.56231028e-01,  4.00552690e-01,\n",
              "         4.96627450e-01,  3.65585834e-01,  3.93852681e-01,\n",
              "         1.48756653e-01,  1.19121507e-01, -8.84617716e-02,\n",
              "         1.19188398e-01, -1.59259170e-01, -1.59498043e-02,\n",
              "        -1.19165316e-01, -1.12607107e-02,  3.36829662e-01,\n",
              "         3.79653633e-01,  1.08324364e-02,  3.58233713e-02,\n",
              "        -1.21563554e-01,  2.94287466e-02,  1.95331186e-01,\n",
              "         2.51535058e-01, -3.94293293e-03, -1.28925294e-02,\n",
              "        -4.41103950e-02,  3.26057881e-01, -5.07812947e-04,\n",
              "         2.29166225e-02, -4.11979407e-02,  5.20538539e-03,\n",
              "        -1.09587654e-01,  1.78050682e-01,  8.27546120e-02,\n",
              "         1.36109918e-01,  3.98670793e-01,  3.43078673e-01,\n",
              "        -8.28449801e-02,  1.98617488e-01, -1.62777454e-01,\n",
              "        -1.32043675e-01, -7.79833347e-02, -5.04653938e-02,\n",
              "         6.42530322e-02,  2.04070613e-01,  4.69409153e-02,\n",
              "         4.10126932e-02,  5.99959120e-03,  4.54045415e-01,\n",
              "        -8.84358510e-02,  2.96464890e-01,  1.08280942e-01,\n",
              "         7.07320869e-02,  1.66770563e-01,  3.38789344e-01,\n",
              "        -4.90982383e-02,  2.09295988e-01, -6.93437532e-02,\n",
              "         3.61780941e-01,  1.06187284e-01,  1.38260752e-01,\n",
              "         5.41641824e-02,  2.53865272e-01,  1.11763090e-01,\n",
              "         3.72541070e-01,  4.66857627e-02,  2.43289545e-01,\n",
              "        -9.24638063e-02, -3.04493718e-02,  3.51066217e-02,\n",
              "         4.55872595e-01, -2.55771205e-02,  4.00797844e-01,\n",
              "         4.19056654e-01,  2.92210698e-01,  3.99691224e-01,\n",
              "        -4.77937609e-03,  3.87663990e-02,  5.67813069e-02,\n",
              "         6.76058009e-02, -6.23935983e-02,  4.38479446e-02,\n",
              "        -9.34957787e-02,  1.99076664e-02,  6.70101643e-02,\n",
              "         1.40098929e-01,  6.08758293e-02,  3.57556283e-01,\n",
              "         8.47567804e-03,  5.52259386e-02,  1.05572075e-01,\n",
              "         1.78858161e-01,  4.46321398e-01,  4.42753553e-01,\n",
              "         2.33443081e-01,  4.10037637e-01, -9.51568037e-02,\n",
              "         1.68062165e-01, -9.58376229e-02,  2.12941289e-01,\n",
              "        -7.44543076e-02,  4.17830646e-01, -2.61966884e-02,\n",
              "         1.71926960e-01, -5.50055504e-03,  2.98521608e-01,\n",
              "        -7.76780397e-02, -5.33164889e-02,  2.24795774e-01,\n",
              "         2.13003397e-01,  1.81791186e-03,  2.13793829e-01,\n",
              "        -1.43293291e-02,  9.29152966e-03,  4.21423137e-01,\n",
              "         4.22622204e-01,  4.10856038e-01,  4.33353484e-01,\n",
              "         3.45011860e-01,  2.19324529e-01,  3.93204361e-01,\n",
              "         3.93902957e-01,  3.69925886e-01,  1.59030706e-01,\n",
              "         3.84531438e-01,  3.82129192e-01, -1.00664862e-01,\n",
              "        -2.08756495e-02,  3.52373756e-02,  7.40339756e-02,\n",
              "        -2.11449340e-03,  2.90979743e-02,  9.68531221e-02,\n",
              "         1.53682008e-01, -1.02685079e-01, -9.98384356e-02,\n",
              "         2.65303068e-02,  1.30098648e-02]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Max Sum Similarity\n",
        "\n",
        "후보 간의 유사성 최소화, 문서와의 후보 유사성 극대화"
      ],
      "metadata": {
        "id": "DZ6JW4N_rz6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
        "  #문서와 각 키워드들 간의 유사도\n",
        "  distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "  #각 키워드들 간의 유사도\n",
        "  distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
        "  #코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick\n",
        "  words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "  words_vals = [candidates[index] for index in words_idx]\n",
        "  distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "  #각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
        "  min_sim = np.inf\n",
        "  candidate = None\n",
        "  for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "    sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "    if sim < min_sim:\n",
        "      candidate = combination\n",
        "      min_sim = sim\n",
        "\n",
        "  return [words_vals[idx] for idx in candidate]"
      ],
      "metadata": {
        "id": "5Kjmw5jRqcde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#상위 10개 키워드 중에 서로 가장 유사성 낮은 5개 선택\n",
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWB4mRRCtK7-",
        "outputId": "d8fb2a49-1446-4caa-8b42-cb24e3ff4c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['일부 기능 해커', '애플 웹브라우저', '보호 위해 애플', '애플 정부 지원', '아이폰 스파이웨어 취약']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20) #높으면 더 다양"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgPWGn8utWn2",
        "outputId": "39cbba9c-4940-44f3-87b3-bcd0eb1cb7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['아이폰 아이패드 컴퓨터', '해커 정부 경찰', '컴퓨터 보안 취약점', '애플 앞서 작년', '보호 위해 애플']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Maximal Marginal Relevance\n",
        "\n",
        "텍스트 요약 작업에서 중복 최소화, 결과 다양성 극대화\n",
        "\n",
        "문서와 가장 유사한 키워드/키프레이즈 선택. 문서와 유사하고 이미 선택된 키워드/키프레이즈와 유사하지 않은 새로운 후보 반복적 선택"
      ],
      "metadata": {
        "id": "g6w6GAvCtape"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
        "  #문서와 각 키워드들 간의 유사도가 적힌 리스트\n",
        "  word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
        "  #각 키워드들 간의 유사도\n",
        "  word_similarity = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
        "  #문서와 가장 높은 유사도를 가진 키워드의 인덱스 추출\n",
        "  keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "  #가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
        "  candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "  #최고의 키워드는 이미 추출. top_n-1번만큼 아래를 반복\n",
        "  for _ in range(top_n-1):\n",
        "    candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "    target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "    #MMR 계산\n",
        "    mmr = (1-diversity)*candidate_similarities - diversity*target_similarities.reshape(-1,1)\n",
        "    mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "    #keywords & candidates 업데이트\n",
        "    keywords_idx.append(mmr_idx)\n",
        "    candidates_idx.remove(mmr_idx)\n",
        "\n",
        "  return [words[idx] for idx in keywords_idx]"
      ],
      "metadata": {
        "id": "nzugxPEAtpPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIgPYwEwu_qF",
        "outputId": "7f571dad-dba1-4d90-a12a-80524c29e44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['아이폰 컴퓨터 보안', '애플 정부 지원', '일부 기능 해커', '아이폰 스파이웨어 취약', '보호 위해 애플']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)  #높으면 더 다양"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjYC4r9EvHAr",
        "outputId": "601d4750-5339-4a8c-d54b-b232a2bbae2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['아이폰 컴퓨터 보안', '각국 정부 문제', '정치인 언론인 인권', '기기 감염', '기업 임원']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 애플 다른 주제로 키워드 뽑아보기"
      ],
      "metadata": {
        "id": "4GGdqmc1wS2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"인천시 남동구 서창2동 행정복지센터가 애플어린이집으로부터 취약계층을 위한 후원금 150만 원을 전달받았다고 6일 밝혔다.\n",
        "후원금은 지난 5월 어린이집에서 원생을 비롯한 학부모, 교직원, 지역주민이 뜻을 모아 진행한 플리마켓(벼룩시장) 수익금으로, 서창2동 취약계층을 위한 복지기금으로 전액 기탁했다.\n",
        "하현주 애플어린이집 원장은“기부를 통해 아이들이 소외된 이웃과 따뜻한 마음을 나누는 기쁨을 경험하는 소중한 기회가 됐다\"며 우리 동 어려운 이웃을 위해 도움을 보태고자 하반기에도 기부를 추진할 예정이라고 전했다.\n",
        "박미경 서창2동장은“소중한 나눔을 실천해주신 애플어린이집에 감사드리며, 전달받은 후원금은 아동이 많이 거주하는 우리 동 특성에 맞춰 아이들을 위한 복지사업 추진을 위해 가치 있게 쓰겠다.”라고 말했다.\"\"\""
      ],
      "metadata": {
        "id": "KLW3kduYmSrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#형태소 분석기를 통해 명사만 추출할 문서 만들기\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_doc = okt.pos(doc)\n",
        "tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
        "print('품사 태깅 10개만 출력 :', tokenized_doc[:10])\n",
        "print('명사 추출 :', tokenized_nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce8341a-a9b1-41d7-8a19-0d09e08bf808",
        "id": "M2z_apxmwpiw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태깅 10개만 출력 : [('인천', 'Noun'), ('시', 'Noun'), ('남동구', 'Noun'), ('서창', 'Noun'), ('2', 'Number'), ('동', 'Modifier'), ('행정', 'Noun'), ('복지', 'Noun'), ('센터', 'Noun'), ('가', 'Josa')]\n",
            "명사 추출 : 인천 시 남동구 서창 행정 복지 센터 애플 어린이집 취약 계층 위 후 원금 원 전달 후원 금은 지난 어린이집 생 비롯 학부모 교직원 지역 주민 뜻 진행 플리 마켓 벼룩시장 수익금 서창 취약 계층 위 복지 기금 전액 기탁 하현 주 애플 어린이집 원장 기부 통해 아이 소외 이웃 마음 기쁨 경험 기회 며 우리 이웃 위해 도움 보태 고자 하반기 기부 추진 예정 전 박미경 서창 동장 나눔 실천 애플 어린이집 감사 전달 후원 금은 아동 거주 우리 특성 아이 위 복지 사업 추진 위해 가치 말\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#여기서는 사이킷런의 CountVectorizer를 사용하여 단어 추출 (n-gram 추출 쉬움)\n",
        "\n",
        "n_gram_range = (2,3)\n",
        "\n",
        "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
        "candidates = count.get_feature_names_out()\n",
        "\n",
        "print('gram 개수 :', len(candidates))\n",
        "print('gram 다섯개만 출력 :', candidates[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf80457-4be8-4f75-c0b7-946863420ee4",
        "id": "7v5J2cpJwpiy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram 개수 : 145\n",
            "gram 다섯개만 출력 : ['감사 전달' '감사 전달 후원' '거주 우리' '거주 우리 특성' '경험 기회']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#이제 문서와 문서로부터 추출한 키워드들을 SBERT를 통해서 수치화한다\n",
        "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')"
      ],
      "metadata": {
        "id": "tzBbcm-Cwpi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embedding = model.encode([doc])\n",
        "candidate_embeddings = model.encode(candidates)"
      ],
      "metadata": {
        "id": "hdSoooFQwpi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이제 문서와 가장 유사한 키워드 추출.(문서를 대표하기 위한 좋은 키워드)\n",
        "top_n = 5\n",
        "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "print(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32bbe43-60da-473d-f6cc-739c6ad2f57c",
        "id": "RpfE0j-Pwpi4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['후원 금은 아동', '아이 복지 사업', '어린이집 감사 전달', '애플 어린이집 감사', '기부 통해 아이']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Max Sum Similarity\n",
        "\n",
        "후보 간의 유사성 최소화, 문서와의 후보 유사성 극대화"
      ],
      "metadata": {
        "id": "kcact8STwpi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
        "  #문서와 각 키워드들 간의 유사도\n",
        "  distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "  #각 키워드들 간의 유사도\n",
        "  distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
        "  #코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick\n",
        "  words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "  words_vals = [candidates[index] for index in words_idx]\n",
        "  distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "  #각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
        "  min_sim = np.inf\n",
        "  candidate = None\n",
        "  for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "    sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "    if sim < min_sim:\n",
        "      candidate = combination\n",
        "      min_sim = sim\n",
        "\n",
        "  return [words_vals[idx] for idx in candidate]"
      ],
      "metadata": {
        "id": "uuZxsLXiwpi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#상위 10개 키워드 중에 서로 가장 유사성 낮은 5개 선택\n",
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e77c35d-8c98-4f54-90b2-f147fb1d609c",
        "id": "j39geNkfwpi9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['이웃 마음 기쁨', '하현 애플 어린이집', '실천 애플 어린이집', '어린이집 감사', '아이 복지 사업']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20) #높으면 더 다양"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09acbaac-7bea-4cf9-a804-690ba1cbca89",
        "id": "C4ArroBbwpi-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['애플 어린이집 취약', '계층 복지 기금', '어린이집 비롯 학부모', '나눔 실천 애플', '이웃 마음 기쁨']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Maximal Marginal Relevance\n",
        "\n",
        "텍스트 요약 작업에서 중복 최소화, 결과 다양성 극대화\n",
        "\n",
        "문서와 가장 유사한 키워드/키프레이즈 선택. 문서와 유사하고 이미 선택된 키워드/키프레이즈와 유사하지 않은 새로운 후보 반복적 선택"
      ],
      "metadata": {
        "id": "uT2ezXolwpi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
        "  #문서와 각 키워드들 간의 유사도가 적힌 리스트\n",
        "  word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
        "  #각 키워드들 간의 유사도\n",
        "  word_similarity = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
        "  #문서와 가장 높은 유사도를 가진 키워드의 인덱스 추출\n",
        "  keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "  #가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
        "  candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "  #최고의 키워드는 이미 추출. top_n-1번만큼 아래를 반복\n",
        "  for _ in range(top_n-1):\n",
        "    candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "    target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "    #MMR 계산\n",
        "    mmr = (1-diversity)*candidate_similarities - diversity*target_similarities.reshape(-1,1)\n",
        "    mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "    #keywords & candidates 업데이트\n",
        "    keywords_idx.append(mmr_idx)\n",
        "    candidates_idx.remove(mmr_idx)\n",
        "\n",
        "  return [words[idx] for idx in keywords_idx]"
      ],
      "metadata": {
        "id": "RlY41uT7wpjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece6f84e-a89f-4d57-9c82-967c2bd4fe2d",
        "id": "ybYcrRpbwpjB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['기부 통해 아이', '애플 어린이집 감사', '어린이집 감사 전달', '아이 복지 사업', '후원 금은 아동']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)  #높으면 더 다양"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e21d13b-6473-49fe-fb33-65e9ed614415",
        "id": "tzytIfIswpjC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['기부 통해 아이', '인천 남동구 서창', '기탁 하현 애플', '사업 추진', '소외 이웃 마음']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 함수 설정"
      ],
      "metadata": {
        "id": "jA0qoabsrZlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def candidates_fn(doc, n_gram_range):\n",
        "  okt = Okt()\n",
        "\n",
        "  tokenized_doc = okt.pos(doc)\n",
        "  tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
        "\n",
        "  count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
        "  candidates = count.get_feature_names_out()\n",
        "\n",
        "  return candidates"
      ],
      "metadata": {
        "id": "egYAXjWerbFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_fn(doc, candidates):\n",
        "  #이제 문서와 문서로부터 추출한 키워드들을 SBERT를 통해서 수치화한다\n",
        "  model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "  doc_embedding = model.encode([doc])\n",
        "  candidate_embeddings = model.encode(candidates)\n",
        "\n",
        "  return (doc_embedding, candidate_embeddings)"
      ],
      "metadata": {
        "id": "EOcLgty2rdRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keywords_extract(doc_embedding, candidate_embeddings, candidates, top_n):\n",
        "  distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "  keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "\n",
        "  return keywords"
      ],
      "metadata": {
        "id": "8lZZhxR0rel8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, nr_candidates):\n",
        "  #문서와 각 키워드들 간의 유사도\n",
        "  distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "  #각 키워드들 간의 유사도\n",
        "  distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
        "  #코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick\n",
        "  words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "  words_vals = [candidates[index] for index in words_idx]\n",
        "  distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "  #각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
        "  min_sim = np.inf\n",
        "  candidate = None\n",
        "  for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "    sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "    if sim < min_sim:\n",
        "      candidate = combination\n",
        "      min_sim = sim\n",
        "\n",
        "  return [words_vals[idx] for idx in candidate]"
      ],
      "metadata": {
        "id": "1is2Qz2Vrf0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
        "  #문서와 각 키워드들 간의 유사도가 적힌 리스트\n",
        "  word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
        "  #각 키워드들 간의 유사도\n",
        "  word_similarity = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
        "  #문서와 가장 높은 유사도를 가진 키워드의 인덱스 추출\n",
        "  keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "  #가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
        "  candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "  #최고의 키워드는 이미 추출. top_n-1번만큼 아래를 반복\n",
        "  for _ in range(top_n-1):\n",
        "    candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "    target_similarities = np.max(word_similarity[candidates_idx][:/, keywords_idx], axis=1)\n",
        "\n",
        "    #MMR 계산\n",
        "    mmr = (1-diversity)*candidate_similarities - diversity*target_similarities.reshape(-1,1)\n",
        "    mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "    #keywords & candidates 업데이트\n",
        "    keywords_idx.append(mmr_idx)\n",
        "    candidates_idx.remove(mmr_idx)\n",
        "\n",
        "  return [words[idx] for idx in keywords_idx]"
      ],
      "metadata": {
        "id": "SHVCGv9yrhMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = candidates_fn(doc, (2,3))"
      ],
      "metadata": {
        "id": "hmhUjHkiriwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embedding, candidate_embeddings = embedding_fn(doc, candidates)"
      ],
      "metadata": {
        "id": "3iQ_INeYrj7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = keywords_extract(doc_embedding, candidate_embeddings, candidates, 5)\n",
        "keywords"
      ],
      "metadata": {
        "id": "pPaIKLuIrk81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mss_keywords = max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20) #높으면 더 다양\n",
        "mss_keywords"
      ],
      "metadata": {
        "id": "yD2daQY-rmAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmr_keywords = mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)  #높으면 더 다양\n",
        "mmr_keywords"
      ],
      "metadata": {
        "id": "jNvRfal2rm4s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}